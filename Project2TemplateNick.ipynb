{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![for sale image, from https://time.com/5835778/selling-home-coronavirus/](https://api.time.com/wp-content/uploads/2020/05/selling-home-coronavirus.jpg?w=800&quality=85)\n",
    "\n",
    "# Building a Predictive Model for Housing Sales\n",
    "## Using Linear Regression for Mom & Pop Realty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Business Problem\n",
    "\n",
    "For the second phase of Flatiron's Live Data Science program we were tasked with developing a multiple linear regression model. This model would predict the price of houses in King County, Washington using data from the King County reality dataset. We decided to develop this model for a small realty business named \"Mom and Pop Reality\". The goal is to provide an accurate prediction for the price of their client's home before puttting it on the market. Client's will always want to get the most money for their home possible. However, realty firms will quickly find themselves with a poor reputation and out of business if a they are misleading or dishonest in their assessed target price. Assuming the firm is acting on good faith and want to provide an accurate assessment, their prediction model must be flexible to the market to continue being competitive in the market place. <br><br>\n",
    "With these concerns in mind, we set out to explore the features in the data set to design our model, explore correlations between different features and the sale price of the home, and use the features with strong correlations to develop a model to achieve our goal. We made sure to normalize our data using a log transformation and scale our data for consistent analysis. As we concluded our analysis, we discovered the most important features to predicting sale price was the size of the home, the size of the lot, the number of bedrooms, and the condition and grade of the home. We recommend Mom & Pop Realty take these features into consideration when assessing the values of client's homes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "This dataset contains house sale prices for King County,Washington. It includes homes sold between May 2014 and May 2015. The data was gathered from King County GIS Open Data. The data represents different features of homes in King County. The data is widely varied, as is to be expected. The data states when the house was built and if it was renovated as well as the date of sale. The data includes counts on floors, bathrooms and bedrooms. Also included in grade and condition of the home. The data also includes waterfront property designation and data on view on different landmarks from the property. The data inlcudes information on basement, living and lot area. The data also includes information regarding living and lot area of the closest 15 properties. Additionally, there is also locational data including zip code and latitude and longitudinal of the property. \n",
    "\n",
    "Given these features, the target variable will be the sale price of the home, as the goal is to build a model to predict price. We selected features related to the house as the important predictor features. These include continuous data such as the home and lot size, ordinal data such as the number of bedrooms and bathrooms, and categorical data such as the grade and condition of the homes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what data types are in the data?\n",
    "data = pd.read_csv('data/kc_house_data.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for any null values in the data.\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicates from the data set based on their id value.\n",
    "data.drop_duplicates(subset='id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'sqft_basement' is an object type?\n",
    "# exploring the area of basements\n",
    "data['sqft_basement'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '?' is a strange value, we'll make that 0\n",
    "data.loc[data['sqft_basement'] == '?','sqft_basement'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orginally this data was stored as text (object), \n",
    "# we transform the data type to float to use later for anaylsis\n",
    "# check to ensure the \"?\" is now 0.0\n",
    "data['sqft_basement'] = data['sqft_basement'].map(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploring the distribution of the features\n",
    "data.hist(figsize=(20,20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these features look like they could use some log processing. Running a log transformation will help normalize the data for better regression analysis down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running the log transformation. creates a new data frame.\n",
    "datalog = pd.DataFrame()\n",
    "log_cols = ['id','price','sqft_living','sqft_lot','sqft_above', 'sqft_basement']\n",
    "for col in log_cols:\n",
    "    if col == 'id':\n",
    "        datalog[col] = data[col]\n",
    "        continue\n",
    "    datalog[f'{col}_log'] = data[col].map(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merges the orginal and new dataframe\n",
    "data = pd.merge(data,datalog,on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checking the columns to ensure all data is collected.\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of included features\n",
    "\n",
    "Since there were so many available predictors, we decided to take a subset of them. We based this decision on a desire to make the model predictive only on the features of the house itself, and not so much the location information. Furthermore, we did some initial exploration with location information, but it didn't really go anywhere so we scrapped the idea. See the notebook in 'notebooks/Dave/King_County_Map.ipynb'.\n",
    "\n",
    "Firstly, because the price is so skewed, we wanted to use log(price) as our output variable, since it will be more normally distributed and thus will follow the assumptions of linear regression a bit better (residuals will be more normal as well). For downstream understanding purposes, we kept the original 'price' as well.\n",
    "\n",
    "Secondly, we looked at 'grade', 'condition', 'view', 'sqft_living_log', 'sqft_lot_log', 'sqft_above_log', 'sqft_basement_log', 'bedrooms', 'bathrooms', and 'floors' as potentially important predictors of price.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reducing our features to consider for analyzation\n",
    "rel_cols = ['price','price_log', 'grade', 'condition', 'view', 'sqft_living_log',\n",
    "            'sqft_lot_log', 'sqft_above_log', 'sqft_basement_log', \n",
    "            'bedrooms', 'bathrooms', 'floors','sqft_living',\n",
    "            'sqft_lot', 'sqft_above']\n",
    "data = data[rel_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "In this section we will transform and remove some feature to make the data easier to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#looking at some statistical information of the numerical dataframe columms\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the top 15 properties according to each feature\n",
    "for col in data.columns:\n",
    "    print(f'\\n{col}:\\n')\n",
    "    print(data.sort_values(by=col,ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33 bedrooms is pretty crazy and not highly correlated with a high price. Based on the data, we are unsure if this data is a typo or an honest outlier. We decided to drop this property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['bedrooms'] != 33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the value counts for view we see none composes 89.8% of the column.\n",
    "data['view'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['view'].value_counts()[0]/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete 'view' column since not much information given\n",
    "data.drop(columns='view', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we have the new distribution of home prices in the data set\n",
    "fig, ax = plt.subplots(figsize= (12,8))\n",
    "ax = sns.histplot(data[\"price\"]/1000)\n",
    "\n",
    "ax.set_title(\"Home prices in King County\",fontsize=40)\n",
    "ax.set_xlabel(\"Price (in thousands)\",fontsize=30)\n",
    "ax.set_ylabel(\"Number of homes\",fontsize=30)\n",
    "ax.tick_params(labelsize=20)\n",
    "ax.xaxis.set_major_formatter('${x:1.0f}')\n",
    "plt.savefig(\"House_prices.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median house sale price\n",
    "data['price'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming categorical data\n",
    "#### 'Grade' to 4 categories: Low, Average, Above Average, and Excellent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lows including 3 Poor, 5 Fair and 6 Low Average\n",
    "data['grade'].replace('3 Poor','Low Grade', inplace=True)\n",
    "data['grade'].replace('5 Fair','Low Grade', inplace=True)\n",
    "data['grade'].replace('4 Low','Low Grade', inplace=True)\n",
    "data['grade'].replace('6 Low Average','Low Grade', inplace=True)\n",
    "\n",
    "# Average including 7 Average \n",
    "data['grade'].replace('7 Average','Average Grade', inplace=True)\n",
    "\n",
    "# Average Above including 8 Good,9 Better\n",
    "data['grade'].replace('8 Good','Above Average Grade', inplace=True)\n",
    "data['grade'].replace('9 Better','Above Average Grade', inplace=True)\n",
    "\n",
    "# Excellent including 10 Very Good, 11 Excellent,12 Luxury and 13 Mansion\n",
    "data['grade'].replace('10 Very Good','Excellent Grade', inplace=True)\n",
    "data['grade'].replace('11 Excellent','Excellent Grade', inplace=True)\n",
    "data['grade'].replace('12 Luxury','Excellent Grade', inplace=True)\n",
    "data['grade'].replace('13 Mansion','Excellent Grade', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#feature breakdown\n",
    "data['grade'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'Condition' to only 2 categories: Low-Average, and Good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#looking at feature breakdown to decide how to transform the data\n",
    "data['condition'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low-Average including Poor, Fair, Average\n",
    "data['condition'].replace('Poor','Low-Average Condition', inplace=True)\n",
    "data['condition'].replace('Fair','Low-Average Condition', inplace=True)\n",
    "data['condition'].replace('Average','Low-Average Condition', inplace=True)\n",
    "\n",
    "# Good including Good and Very Good\n",
    "data['condition'].replace('Good','Good Condition', inplace=True)\n",
    "data['condition'].replace('Very Good','Good Condition', inplace=True)\n",
    "\n",
    "#data is better distribued when grouped\n",
    "data['condition'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#exploring the means and counts for grades per condition\n",
    "data.groupby(by=['condition','grade']).agg(['mean','count'])['price_log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#exploring the means and counts for grades per condition\n",
    "data.groupby(by=['grade','condition']).agg(['mean','count'])['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (25,10))\n",
    "sns.violinplot(x='grade',y=(data['price']/1000), hue= \"condition\", data = data, palette=\"rocket\", \n",
    "               order=[\"Low Grade\", \"Average Grade\", \"Above Average Grade\", \"Excellent Grade\"], \n",
    "               hue_order = [\"Low-Average Condition\", \"Good Condition\"])\n",
    "\n",
    "ax.set_title(\"Grade and Condition of Homes\",fontsize=50)\n",
    "ax.set_xlabel(\"Grade\",fontsize=40)\n",
    "ax.set_ylabel(\"Price (in Thousands)\",fontsize=40)\n",
    "ax.tick_params(labelsize=30)\n",
    "ax.yaxis.set_major_formatter('${x:1.0f}')\n",
    "ax.legend(loc=\"upper left\", borderaxespad=0., title = \"Condition\",fontsize=20, title_fontsize=30)\n",
    "plt.savefig(\"Grade_Condition.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations of Grad and conditions of Homes\n",
    "Home price increases as grade improves <br>\n",
    "Home price increases as condition improves <br>\n",
    "Home price increases as condition improves within grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding the categorical data\n",
    "He break out our categorical featsures (Condition and Grade) into individaul columns using SkLearn's Onehotencoding function and drop the first column to avoid multicollinearity issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = data[[\"condition\"]]\n",
    "ohe = OneHotEncoder(categories=\"auto\", handle_unknown=\"error\", sparse=False)\n",
    "ohe.fit(cond)\n",
    "cond_encod = ohe.transform(cond)\n",
    "cond_encod = pd.DataFrame(\n",
    "    # Pass in NumPy array\n",
    "    cond_encod,\n",
    "    # Set the column names to the categories found by OHE\n",
    "    columns=ohe.categories_[0],\n",
    "    # Set the index to match X_train's index\n",
    "    index= data.index\n",
    ")\n",
    "cond_encod.drop(columns='Low-Average Condition', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade = data[[\"grade\"]]\n",
    "ohe = OneHotEncoder(categories=\"auto\",handle_unknown=\"ignore\", sparse=False)\n",
    "ohe.fit(grade)\n",
    "grade_encod = ohe.transform(grade)\n",
    "grade_encod = pd.DataFrame(\n",
    "    # Pass in NumPy array\n",
    "    grade_encod,\n",
    "    # Set the column names to the categories found by OHE\n",
    "    columns=ohe.categories_[0],\n",
    "    # Set the index to match X_train's index\n",
    "    index= data.index\n",
    ")\n",
    "grade_encod.drop(columns='Average Grade', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drops the orginal columns because the data wee need is now split into columns\n",
    "data = pd.concat([data, cond_encod, grade_encod], axis=1)\\\n",
    "        .drop(columns=['condition','grade'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing sqft_basement to a binomial variable \"has_basement\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace sqft_basement with has_basement: True (1) / False (0)\n",
    "data['sqft_basement_log'] = data['sqft_basement_log'].map(lambda x: 1 if x > 0 else 0)\n",
    "data.rename(columns={'sqft_basement_log':'has_basement'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pair plot visually exploring the relationship between the features\n",
    "# sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploring the distributions of our data\n",
    "fig, axes = plt.subplots(8,3, figsize=(30,30))\n",
    "for i, col in enumerate(data.columns):\n",
    "    sns.histplot(data=data, x=col, kde=True, ax=axes[i//3,i%3]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploring the correlations between the price and features\n",
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#an organized series showing the correlations to price\n",
    "datacorrp = data[['price_log', 'has_basement', 'bedrooms', 'bathrooms', 'floors',\n",
    "       'sqft_living', 'sqft_lot', 'sqft_above',\n",
    "       'Good Condition', 'Above Average Grade', 'Excellent Grade',\n",
    "       'Low Grade']]\n",
    "datacorr = datacorrp.corr().sort_values('price_log',ascending=False)['price_log']\n",
    "datacorr = datacorr.drop(index=['price_log'])\n",
    "datacorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "sns.barplot(x=datacorr.values, y=datacorr.index, orient='h', color='b')\n",
    "ylabels = [item.capitalize() for item in datacorr.index]\n",
    "ylabels = [item.split('_') for item in ylabels]\n",
    "ylabels = [' '.join(item) for item in ylabels]\n",
    "ax.set_yticklabels(ylabels, size=16)\n",
    "ax.tick_params(axis='x', which='major', labelsize=18)\n",
    "ax.set_xlabel('Correlation to Sale Price', size=18)\n",
    "ax.set_ylabel('House Feature', size=18)\n",
    "ax.set_title('House Feature Correlations', size=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#exploring the correlations between features to look for multicollinearity issues\n",
    "datafeat = data.drop(columns=['price','price_log'])\n",
    "datafeat.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzing the high correlated pairs\n",
    "datafeat = data.drop(columns=['price_log','price'])\n",
    "dtfc = datafeat.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n",
    "\n",
    "dtfc['col_pairs'] = list(zip(dtfc.level_0,dtfc.level_1))\n",
    "dtfc['same'] = dtfc['col_pairs'].map(lambda x: (x[0] in x[1]) or (x[1] in x[0]))\n",
    "dtfc['col_pairs'] = dtfc['col_pairs'].map(lambda x:sorted(list(x)))\n",
    "dtfc.set_index(['col_pairs'],inplace=True)\n",
    "dtfc = dtfc[dtfc['same'] == False]\n",
    "dtfc.drop(columns=['level_0','level_1','same'],inplace=True)\n",
    "dtfc.columns = ['C']\n",
    "dtfc.drop_duplicates(inplace=True)\n",
    "dtfc.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Because these is some clear multicollinearity occurring between house features, we decided that one way to get around this was to add features that combined them in logical ways. This would help deal with the dependence of variables on each other, and maybe give some heightened insight into how these variables are important to the sale price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bedrooms per sqfoot of living space\n",
    "data[\"bedroom/sqft_above_log\"] = data[\"bedrooms\"] / data[\"sqft_above_log\"]\n",
    "#bathrooms per sqfoot of living space \n",
    "data[\"bathrooms/sqft_above_log\"] = data[\"bathrooms\"] / data[\"sqft_above_log\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of data correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the column names for our combined data sets\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring direct linear regressions on price\n",
    "We will not explore logged data for clarity and understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = stats.linregress(data[\"sqft_living\"],data['price'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "ax = sns.regplot(x=(data[\"sqft_above\"]), y=(data['price']/1000),\n",
    "                 line_kws={'label':\"${0:.2f}/Square foot increase in home space\".format(slope)})\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bathtemp = data[['price','bathrooms']]\n",
    "bathtemp.loc[bathtemp['bathrooms'] <= 1,'bathrooms'] = 1\n",
    "bathtemp.loc[(bathtemp['bathrooms'] > 1) & (bathtemp['bathrooms'] <= 2),'bathrooms'] = 2\n",
    "bathtemp.loc[(bathtemp['bathrooms'] > 2) & (bathtemp['bathrooms'] <= 3),'bathrooms'] = 3\n",
    "bathtemp.loc[(bathtemp['bathrooms'] > 3) & (bathtemp['bathrooms'] <= 4),'bathrooms'] = 4\n",
    "bathtemp.loc[(bathtemp['bathrooms'] > 4) & (bathtemp['bathrooms'] <= 5),'bathrooms'] = 5\n",
    "bathtemp.loc[(bathtemp['bathrooms'] > 5) & (bathtemp['bathrooms'] <= 6),'bathrooms'] = 6\n",
    "bathtemp.loc[(bathtemp['bathrooms'] > 6) & (bathtemp['bathrooms'] <= 7),'bathrooms'] = 7\n",
    "bathtemp.loc[(bathtemp['bathrooms'] > 7) & (bathtemp['bathrooms'] <= 8),'bathrooms'] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (35,10))\n",
    "\n",
    "\n",
    "sns.violinplot(x=bathtemp['bathrooms'],y=(bathtemp['price']/1000), data = bathtemp, \n",
    "               palette=\"rocket\")\n",
    "\n",
    "ax.set_title(\"Number of bathrooms effect on home price\",fontsize=50)\n",
    "ax.set_xlabel(\"Bathrooms\",fontsize=40)\n",
    "ax.set_ylabel(\"Price (in Thousands)\",fontsize=40)\n",
    "ax.tick_params(labelsize=30)\n",
    "ax.yaxis.set_major_formatter('${x:1.0f}')\n",
    "plt.savefig(\"bathrooms.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (25,10))\n",
    "sns.violinplot(x='bedrooms',y=(data['price']/1000), data = data, palette=\"rocket\")\n",
    "\n",
    "ax.set_title(\"Number of bedrooms effect on home price\",fontsize=50)\n",
    "ax.set_xlabel(\"Bedrooms\",fontsize=40)\n",
    "ax.set_ylabel(\"Price (in Thousands)\",fontsize=40)\n",
    "ax.tick_params(labelsize=30)\n",
    "ax.yaxis.set_major_formatter('${x:1.0f}')\n",
    "plt.savefig(\"bedrooms.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split the data\n",
    "Here we split the data into a test and train set. We will fit and transform the training data and later fit the training data to analyze. We will be using he log transformed data to utilize the more normative distribution of the data. We will additionally be scaling our data to allow the model to weigh the features equally bcasue they will be on the same scale and we will be able to compare the feature coefficients bewteen one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X will b all of the features of the data set to analyze. \"price_log\" will be the target variable.\n",
    "X = data.drop(columns=['price_log','price'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X, data['price_log'], test_size=0.33, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the descriptive statistics of our x training data.\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the descriptive statistics of our x test data.\n",
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the descriptive statistics of our y training data.\n",
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the descriptive statistics of our y test data.\n",
    "y_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we use SkLearn's Standard Scaler to scale all of our data to the same scale\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking a look at the scaled X trained data\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Understanding\n",
    "\n",
    "The simplest model will predict the average price for all homes regardless of their features. This is the baseline model we use to compare all future models against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_mean = y_train.mean()\n",
    "baseline_train_pred = [train_target_mean] * len(y_train)\n",
    "baseline_test_pred = [train_target_mean] * len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to evaluate the models and their predicted sale prices vs. the actual sale prices\n",
    "def evaluate(y_tr, y_te, y_tr_pr, y_te_pr, log=True):\n",
    "    '''\n",
    "    Evaluates the error between the model predictions and the real values for both\n",
    "    training and test sets.\n",
    "    \n",
    "    Arguments:\n",
    "    y_tr - array-like\n",
    "        Actual values for output variable, for the training set\n",
    "    y_tr_pr - array-like\n",
    "        Predicted values for output variable, for the training set\n",
    "    y_te - array-like\n",
    "        Actual values for output variable, for the test set\n",
    "    y_te_pr - array-like\n",
    "        Predicted values for output variable, for the test set\n",
    "    log=True\n",
    "        If true, \n",
    "    Returns:\n",
    "    R2 scores for Train and Test sets\n",
    "    RMSE for Train and Test sets\n",
    "    MAE for Train and Test sets\n",
    "    '''\n",
    "    if log == True:\n",
    "        y_tr = np.exp(y_tr)\n",
    "        y_te = np.exp(y_te)\n",
    "        y_tr_pr = np.exp(y_tr_pr)\n",
    "        y_te_pr = np.exp(y_te_pr)\n",
    "        \n",
    "    # residuals\n",
    "    train_res = y_tr - y_tr_pr\n",
    "    test_res = y_te - y_te_pr\n",
    "    \n",
    "    print(f'Train R2 score: {r2_score(y_tr, y_tr_pr)} ')\n",
    "    print(f'Test R2 score: {r2_score(y_te, y_te_pr)} ')\n",
    "    print('<><><><><>')\n",
    "    print(f'Train RMSE: ${mean_squared_error(y_tr, y_tr_pr, squared=False):,.2f} ')\n",
    "    print(f'Test RMSE: ${mean_squared_error(y_te, y_te_pr, squared=False):,.2f} ')\n",
    "    print('<><><><><>')\n",
    "    print(f'Train MAE: ${mean_absolute_error(y_tr, y_tr_pr):,.2f} ')\n",
    "    print(f'Test MAE: ${mean_absolute_error(y_te, y_te_pr):,.2f} ')\n",
    "    \n",
    "\n",
    "    \n",
    "    # scatter plot of residuals\n",
    "    print(\"\\nScatter of residuals:\")\n",
    "    plt.scatter(y_tr_pr, train_res, label='Train')\n",
    "    plt.scatter(y_te_pr, test_res, label='Test')\n",
    "    plt.axhline(y=0, color='purple', label='0')\n",
    "    plt.xlabel(\"Predicted Price\")\n",
    "    plt.ylabel(\"Residual Price\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"QQ Plot of residuals:\")\n",
    "    fig, ax = plt.subplots()\n",
    "    sm.qqplot(train_res, ax=ax, marker='.', color='r', label='Train', alpha=0.3, line='s')\n",
    "    sm.qqplot(test_res, ax=ax,  marker='.', color='g', label='Test', alpha=0.3)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(y_train, y_test, baseline_train_pred, baseline_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This very simple model will predict the average of the training data set prices and will not utilize any independent variable. This will result in the residuals being the same. As you can see the training and test residuals are almost directly over laid on one another. Which is predicatble based on the paramaters of our current model. This is not a practical model for the scope of our project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Describe and justify the process for analyzing or modeling the data.\n",
    "\n",
    "Questions to consider:\n",
    "\n",
    "- How did you analyze the data to arrive at an initial approach?\n",
    "- How did you iterate on your initial approach to make it better?\n",
    "- Why are these choices appropriate given the data and the business problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smols(X,y,cols=None):\n",
    "    '''\n",
    "    Uses Linear Regression to find a best fit given desired features.\n",
    "    Arguments:\n",
    "    X - dataframe\n",
    "        Input features and values\n",
    "    y - array-like\n",
    "        Output values\n",
    "    cols=None - list\n",
    "        List of features to be included from the X dataframe\n",
    "    Returns: OLS model. Use smols().summary() to view summary\n",
    "    '''\n",
    "    Xcol = X[cols]\n",
    "    shmod = sm.OLS(endog=y, exog=sm.add_constant(Xcol)).fit()\n",
    "    return shmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linpreds(X_tr_scaled, y_tr, X_te_scaled):\n",
    "    '''\n",
    "    Uses Linear Regression to generate output predictions given training and test inputs.\n",
    "    Arguments:\n",
    "    X_tr_scaled - dataframe\n",
    "        Input variables and values for the training set\n",
    "    y_tr - array-like\n",
    "        Actual values for output variable, for the training set\n",
    "    X_te_scaled - dataframe\n",
    "        Input variables and values for the test set\n",
    "    Returns:\n",
    "    Output (y) prediction arrays:\n",
    "        train, test\n",
    "    '''\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_tr_scaled, y_tr)\n",
    "    return lr.predict(X_tr_scaled), lr.predict(X_te_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This very simple, substandard model is a simple linear regression model between the space in a home and the price of the home. This was the strongest correlation we found in our initial analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['sqft_living_log']\n",
    "smols(X_train,y_train,cols).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is better than the baseline model, because it is taking one independent variable into the model compared to none in our base line model. The model is accounting for 43.2% variance in the model. The p-value is less than or alpha of 0.05 which implies significance. After scaling the data back the coefficient for the log square foot of the living space states acoounts for X amount of dollars of change for every 1 square foot increase in the home. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Iterations\n",
    "\n",
    "Now that we have a 'better' model, we can start to add extra features that seem to add predictive value to the model. These are given by the correlation values generated earlier. We'll start with bathrooms and then add the grade categories. Sqft_above is ignored for now because it has some multicollinearity (independence) issues with sqft_living_log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smols(X_train_scaled,y_train,\\\n",
    "      cols=['sqft_living_log','bathrooms']).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smols(X_train_scaled,y_train,\\\n",
    "      cols=['sqft_living_log','bathrooms','Above Average Grade', \n",
    "            'Excellent Grade', 'Low Grade']).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model analyzes how home size, number of bathrooms, and grade interact to predict the home price. The r squred value has risen to 53.3% prediction in variance. Each of the pvalues is lower than alpha implying significance, except bathrooms, which is super high at 78.5%. This is surprising, given how strongly it alone is correlated with price. The top predictors are 'sqft_living_log' and 'Excellent Grade', which makes sense. Bigger homes in better quality should mean higher sale prices. Our condition number is low, meaning there are minimal issues with multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepwise function\n",
    "\n",
    "To find the best combination of features that result in the lowest error in predicted price, we built a forward-backward optimization function.\n",
    "\n",
    "In the forward step, it takes a model with certain included features, and generates a Series of R2 values associated with models (using training data) that include the initial features and one extra feature among those remaining. The best R2 value (for test data) for the new model is compared to the R2 value (for test data) for the initial model. If it is higher, the new feature is added to the included features for the next step and next iteration.\n",
    "\n",
    "In the backward step, after adding each new variable to the included-feature model, the algorithm will generate a Series of RMSE values associated with models that remove one variable from the included-feature model. In that list, the lowest RMSE is compared to the RMSE of the full included-feature model, and if it is lower, that feature is removed from the included-feature model for the next iteration. \n",
    "\n",
    "The algorithm proceeds in an iterative fashion until no features are added or removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepwise_selection(X_tr, y_tr, X_te=None, y_te=None,\n",
    "                       initial_list=[], no_use=[], \n",
    "                       verbose=True):\n",
    "    \"\"\"\n",
    "    Perform a forward-backward feature selection \n",
    "    based on R2 (forward) and RMSE (backward) from sklearn\n",
    "    Arguments:\n",
    "        X - pandas.DataFrame with candidate features\n",
    "        y - list-like with the target\n",
    "        X_tr - pandas.DataFrame with training candidate features\n",
    "        y_tr - list-like with the training target\n",
    "        X_te - pandas.DataFrame with test candidate features\n",
    "        y_te - list-like with the test target\n",
    "        initial_list - list of features to start with (column names of X_tr)\n",
    "        verbose - whether to print the sequence of inclusions and exclusions\n",
    "    Returns: list of selected features \n",
    "    \"\"\"\n",
    "    included = list(set(initial_list))\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step with R2\n",
    "        # add feature if the resulting test R2 >= previous test R2\n",
    "        on_hold = []\n",
    "        excluded = list(set(X_tr.columns)-set(no_use)-set(included)-set(on_hold))\n",
    "        new_r2 = pd.Series(index=excluded, dtype='float64')\n",
    "        for new_column in excluded:\n",
    "            trpred, tepred = linpreds(X_tr[included+[new_column]], y_tr, \n",
    "                                      X_te[included+[new_column]])\n",
    "            new_r2[new_column] = r2_score(y_te, tepred)\n",
    "        best_r2 = new_r2.max()\n",
    "        if best_r2 > r2_score(y_te, tepred):\n",
    "            best_feature = new_r2.idxmax()\n",
    "            included.append(best_feature)\n",
    "            try:\n",
    "                on_hold.pop()\n",
    "            except:\n",
    "                pass\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                print('Add  {:30} with r2: {:.6}'.format(best_feature, best_r2))\n",
    "\n",
    "        # backward step with RMSE\n",
    "        trpred, tepred = linpreds(X_tr[included], y_tr, X_te[included])\n",
    "        y_te_unl, tepred_unl = np.exp(y_te), np.exp(tepred)\n",
    "        rmse_pre = mean_squared_error(y_te_unl, tepred_unl, squared=False)\n",
    "        print('Before removal RMSE: {:.2f}'.format(rmse_pre))\n",
    "        rmses = pd.Series(index=included, dtype='float64')\n",
    "        for column in included:\n",
    "            trpred, tepred = linpreds(X_tr[list(set(included)-set(column))], y_tr, \n",
    "                                      X_te[list(set(included)-set(column))])\n",
    "            y_te_unl, tepred_unl = np.exp(y_te), np.exp(tepred)\n",
    "            rmses[column] = mean_squared_error(y_te_unl, tepred_unl, squared=False)\n",
    "        lowest_rmse = rmses.min()\n",
    "        if lowest_rmse < rmse_pre:\n",
    "            changed=True\n",
    "            bad_feature = rmses.idxmin()\n",
    "            on_hold.append(bad_feature)\n",
    "            included.remove(bad_feature)\n",
    "            if verbose:\n",
    "                print('Drop {:30} with RMSE {:.2f}'.format(bad_feature, lowest_rmse))\n",
    "        else:\n",
    "            if verbose:\n",
    "                print('Keep {:30} with RMSE {:.2f}'.format(best_feature, lowest_rmse))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stepwise_selection(X_train_scaled, y_train, X_test_scaled, y_test,  \\\n",
    "                   initial_list=['sqft_living_log', 'Low Grade',\n",
    "                                 'Excellent Grade','Above Average Grade'], \n",
    "                   no_use=['sqft_living','sqft_lot','sqft_above']) # don't include non-log features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Final' Model\n",
    "\n",
    "In the end, you'll arrive at a 'final' model - aka the one you'll use to make your recommendations/conclusions. This likely blends any group work. It might not be the one with the highest scores, but instead might be considered 'final' or 'best' for other reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relcol =['Excellent Grade',\n",
    " 'Low Grade',\n",
    " 'Above Average Grade',\n",
    " 'sqft_living_log',\n",
    " 'has_basement',\n",
    " 'Good Condition',\n",
    " 'sqft_lot_log']\n",
    "smols(X_train_scaled, y_train, cols=relcol).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xftr, Xfte = X_train_scaled[relcol], X_test_scaled[relcol]\n",
    "trp, tep = linpreds(Xftr, y_train, Xfte)\n",
    "evaluate(y_train, y_test, trp, tep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final model resulted in a $152,7285.76 average variation from observed sales prices and ended with a R squared value of .555 meaning our model accounts for a 55.5% variance in sales price. All of the featurres have a pvalue of lesss than 0.05, which implies all features are significant to the model. Our condition number is less than a 5, meaning there is little to multicollinearity issues. The scatter plot of the residuals does display some heteroskedasticity. Based on the QQ plot, the residuals are slightly skewed to the right. It is interesting to note, none of the engineered features, the number of bathrooms did not make it in to the final model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend Mom & Pop Realty use the grade and condition of the house if the house has a basement, the size of the house and size of the property to predict the price of the a clients home. The strongest predictors is House Square Footage, where a 1% increase in Home quare footage translates to an increase in 0.22% sale price. The next strogest predictor is the grade of the house, specifically, where the house has an excellent grade. Homes with an excellent grade has a 20.1% higher sales price than that of a home with an average grade. We understand this model is incomplete and the level of bias in the model reduces the overall effectiveness. Our final model does not included location data. Adding those features to the model may help the model's bias and heteroskedasticity issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
